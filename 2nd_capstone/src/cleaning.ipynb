{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c1196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STARTING ZOMATO DELIVERY DATA PREPARATION\n",
      "============================================================\n",
      " Using sample data (11 rows from your example)\n",
      "\n",
      "============================================================\n",
      " STEP 2: CLEANING DATE/TIME COLUMNS\n",
      "============================================================\n",
      "\n",
      "üîß Cleaning 'Order_Date' column...\n",
      " Converted to datetime. Sample: 2022-02-12 00:00:00\n",
      "\n",
      "üîß Creating datetime columns...\n",
      " Created datetime columns.\n",
      "   Preparation time stats:\n",
      "   Min: 5.0 min\n",
      "   Max: 15.0 min\n",
      "   Avg: 11.4 min\n",
      "\n",
      "üîß Extracting temporal features...\n",
      " Extracted 6 unique hours\n",
      "   Weekend orders: 5 / 11\n",
      "\n",
      "============================================================\n",
      "STEP 3: CALCULATING HAVERSINE DISTANCE\n",
      "============================================================\n",
      "üìè Calculating distances...\n",
      " Distance calculation complete!\n",
      "\n",
      " Distance Statistics (in KM):\n",
      "   Min Distance: 2.93 km\n",
      "   Max Distance: 19.40 km\n",
      "   Mean Distance: 10.64 km\n",
      "   Median Distance: 10.76 km\n",
      "\n",
      " Distance Distribution:\n",
      "Distance_Category\n",
      "Very Close (<2km)    0\n",
      "Close (2-5km)        2\n",
      "Medium (5-10km)      2\n",
      "Far (10-20km)        7\n",
      "Very Far (>20km)     0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "STEP 4: HANDLING MISSING VALUES\n",
      "============================================================\n",
      "\n",
      " Missing Values Before Cleaning:\n",
      "                     Missing_Count  Missing_Percent\n",
      "multiple_deliveries              1         9.090909\n",
      "\n",
      "üîß Handling 'multiple_deliveries' column...\n",
      "   Filled 1 missing values with 0\n",
      "\n",
      " Missing Values After Cleaning:\n",
      "All missing values handled successfully!\n",
      "\n",
      "============================================================\n",
      " STEP 5: ENCODING CATEGORICAL VARIABLES\n",
      "============================================================\n",
      " Categorical columns to encode: ['Weather_conditions', 'Road_traffic_density', 'Type_of_order', 'Type_of_vehicle', 'Festival', 'City']\n",
      "\n",
      "üî¢ Ordinal encoding 'Road_traffic_density': {'Low': 0, 'Medium': 1, 'High': 2, 'Jam': 3}\n",
      "\n",
      "üî¢ Ordinal encoding 'Vehicle_condition': {0: 0, 1: 1, 2: 2}\n",
      "\n",
      "üî¢ Ordinal encoding 'multiple_deliveries': {0: 0, 1: 1, 2: 2, 3: 3}\n",
      "\n",
      "üé≠ One-hot encoding for: ['Weather_conditions', 'Type_of_order', 'Type_of_vehicle', 'City', 'Festival', 'Time_Period']\n",
      "   'Weather_conditions' ‚Üí 5 categories\n",
      "   'Type_of_order' ‚Üí 4 categories\n",
      "   'Type_of_vehicle' ‚Üí 3 categories\n",
      "   'City' ‚Üí 2 categories\n",
      "   'Festival' ‚Üí 1 categories\n",
      "   'Time_Period' ‚Üí 3 categories\n",
      "\n",
      " Creating binary flags...\n",
      "\n",
      " Encoding Summary:\n",
      "   Road_traffic_density: Ordinal ‚Üí {'Low': 0, 'Medium': 1, 'High': 2, 'Jam': 3}\n",
      "   Vehicle_condition: Ordinal ‚Üí {0: 0, 1: 1, 2: 2}\n",
      "   multiple_deliveries: Ordinal ‚Üí {0: 0, 1: 1, 2: 2, 3: 3}\n",
      "   Weather_conditions: One-Hot ‚Üí 5 categories\n",
      "   Type_of_order: One-Hot ‚Üí 4 categories\n",
      "   Type_of_vehicle: One-Hot ‚Üí 3 categories\n",
      "   City: One-Hot ‚Üí 2 categories\n",
      "   Festival: One-Hot ‚Üí 1 categories\n",
      "   Time_Period: One-Hot ‚Üí 3 categories\n",
      "\n",
      " Created 19 new encoded features\n",
      "   Total features: 56\n",
      "\n",
      "============================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "============================================================\n",
      "\n",
      " Final Dataset Shape: (11, 56)\n",
      " Columns (56 total):\n",
      "\n",
      "    Numeric columns (27):\n",
      "      ['Delivery_person_Age', 'Delivery_person_Ratings', 'Restaurant_latitude', 'Restaurant_longitude', 'Delivery_location_latitude', 'Delivery_location_longitude', 'Vehicle_condition', 'multiple_deliveries', 'Time_taken (min)', 'Preparation_Time_Min']\n",
      "      ... and 17 more\n",
      "\n",
      "   Categorical columns (11):\n",
      "      ['ID', 'Delivery_person_ID', 'Time_Orderd', 'Time_Order_picked', 'Weather_conditions', 'Road_traffic_density', 'Type_of_order', 'Type_of_vehicle', 'Festival', 'City', 'Order_DayOfWeek_Name']\n",
      "\n",
      "   Datetime columns (3):\n",
      "      ['Order_Date', 'Order_DateTime', 'Pickup_DateTime']\n",
      "\n",
      " Prepared data saved to: zomato_delivery_prepared.csv\n",
      "\n",
      "============================================================\n",
      "KEY FEATURES READY FOR MODELING\n",
      "============================================================\n",
      "\n",
      " Correlation with Target (Time_taken):\n",
      "\n",
      "Top correlated features:\n",
      "   Order_Minute                   : +0.533\n",
      "   Delivery_person_Age            : +0.504\n",
      "   Order_Year                     : +nan\n",
      "   Distance_KM                    : +0.508\n",
      "   Manhattan_Distance_KM          : +0.507\n",
      "   Road_traffic_density_Encoded   : +0.497\n",
      "   multiple_deliveries            : +0.401\n",
      "   multiple_deliveries_Encoded    : +0.401\n",
      "   Order_Hour                     : +0.391\n",
      "   Order_Day                      : +0.371\n",
      "\n",
      "============================================================\n",
      " SAMPLE OF PREPARED DATA\n",
      "============================================================\n",
      "   Distance_KM  Preparation_Time_Min  Order_Hour  Is_Weekend  \\\n",
      "0    10.280582                  15.0          21           1   \n",
      "1     6.242319                  10.0          14           1   \n",
      "2    13.787860                  10.0          17           0   \n",
      "3     2.930258                  10.0           9           1   \n",
      "4    19.396618                  15.0          19           0   \n",
      "\n",
      "   Road_traffic_density_Encoded  Bad_Weather  Is_Rush_Hour  Time_taken (min)  \n",
      "0                             3            1             0                46  \n",
      "1                             2            1             0                23  \n",
      "2                             1            1             1                21  \n",
      "3                             0            1             1                20  \n",
      "4                             3            1             1                41  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ZOMATO DELIVERY TIME PREDICTION - PHASE 1: DATA PREPARATION\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD THE DATA\n",
    "# ============================================================================\n",
    "\n",
    "def load_data(file_path='Zomato_Dataset.csv'):\n",
    "    \"\"\"\n",
    "    Load the dataset and display basic information\n",
    "    \"\"\"\n",
    "    print(\" Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"\\n Dataset Shape: {df.shape}\")\n",
    "    print(f\" Columns: {list(df.columns)}\")\n",
    "    print(f\"\\n First 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\n Dataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(f\"\\n Basic Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: CLEAN DATE/TIME COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "def clean_datetime_columns(df):\n",
    "    \"\"\"\n",
    "    Clean and standardize all date/time columns\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" STEP 2: CLEANING DATE/TIME COLUMNS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2.1: Fix inconsistent date formats in 'Order_Date'\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\nüîß Cleaning 'Order_Date' column...\")\n",
    "    \n",
    "    def parse_date(date_str):\n",
    "        \"\"\"Handle multiple date formats\"\"\"\n",
    "        try:\n",
    "            # Try format: DD/MM/YYYY or D/M/YYYY\n",
    "            if '/' in str(date_str):\n",
    "                return datetime.strptime(str(date_str), '%d/%m/%Y')\n",
    "            # Try format: DD-MM-YYYY\n",
    "            elif '-' in str(date_str):\n",
    "                return datetime.strptime(str(date_str), '%d-%m-%Y')\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    \n",
    "    df_clean['Order_Date'] = df_clean['Order_Date'].apply(parse_date)\n",
    "    \n",
    "    print(f\" Converted to datetime. Sample: {df_clean['Order_Date'].iloc[0]}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2.2: Combine date with time columns\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\nüîß Creating datetime columns...\")\n",
    "    \n",
    "    # Create order datetime\n",
    "    df_clean['Order_DateTime'] = pd.to_datetime(\n",
    "        df_clean['Order_Date'].astype(str) + ' ' + df_clean['Time_Orderd'],\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Create pickup datetime\n",
    "    df_clean['Pickup_DateTime'] = pd.to_datetime(\n",
    "        df_clean['Order_Date'].astype(str) + ' ' + df_clean['Time_Order_picked'],\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2.3: Calculate preparation time\n",
    "    # ------------------------------------------------------------------------\n",
    "    df_clean['Preparation_Time_Min'] = (\n",
    "        df_clean['Pickup_DateTime'] - df_clean['Order_DateTime']\n",
    "    ).dt.total_seconds() / 60\n",
    "    \n",
    "    # Handle negative preparation times (if any)\n",
    "    df_clean['Preparation_Time_Min'] = df_clean['Preparation_Time_Min'].clip(lower=0)\n",
    "    \n",
    "    print(f\" Created datetime columns.\")\n",
    "    print(f\"   Preparation time stats:\")\n",
    "    print(f\"   Min: {df_clean['Preparation_Time_Min'].min():.1f} min\")\n",
    "    print(f\"   Max: {df_clean['Preparation_Time_Min'].max():.1f} min\")\n",
    "    print(f\"   Avg: {df_clean['Preparation_Time_Min'].mean():.1f} min\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2.4: Extract temporal features\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\nüîß Extracting temporal features...\")\n",
    "    \n",
    "    # Time of day features\n",
    "    df_clean['Order_Hour'] = df_clean['Order_DateTime'].dt.hour\n",
    "    df_clean['Order_Minute'] = df_clean['Order_DateTime'].dt.minute\n",
    "    \n",
    "    # Day features\n",
    "    df_clean['Order_Day'] = df_clean['Order_DateTime'].dt.day\n",
    "    df_clean['Order_Month'] = df_clean['Order_DateTime'].dt.month\n",
    "    df_clean['Order_Year'] = df_clean['Order_DateTime'].dt.year\n",
    "    df_clean['Order_DayOfWeek'] = df_clean['Order_DateTime'].dt.dayofweek  # Monday=0\n",
    "    df_clean['Order_DayOfWeek_Name'] = df_clean['Order_DateTime'].dt.day_name()\n",
    "    \n",
    "    # Time period categories\n",
    "    df_clean['Time_Period'] = pd.cut(\n",
    "        df_clean['Order_Hour'],\n",
    "        bins=[0, 6, 12, 18, 24],\n",
    "        labels=['Night', 'Morning', 'Afternoon', 'Evening'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Weekend flag\n",
    "    df_clean['Is_Weekend'] = df_clean['Order_DayOfWeek'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    print(f\" Extracted {df_clean['Order_Hour'].nunique()} unique hours\")\n",
    "    print(f\"   Weekend orders: {df_clean['Is_Weekend'].sum()} / {len(df_clean)}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: CALCULATE HAVERSINE DISTANCE\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_haversine_distance(df):\n",
    "    \"\"\"\n",
    "    Calculate distance between restaurant and delivery location using Haversine formula\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: CALCULATING HAVERSINE DISTANCE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_dist = df.copy()\n",
    "    \n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"\n",
    "        Calculate the great circle distance between two points \n",
    "        on the earth (specified in decimal degrees)\n",
    "        \n",
    "        Returns distance in kilometers\n",
    "        \"\"\"\n",
    "        # Convert decimal degrees to radians\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        \n",
    "        # Haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "        \n",
    "        # Radius of earth in kilometers (mean radius = 6371 km)\n",
    "        radius = 6371.0\n",
    "        \n",
    "        return radius * c\n",
    "    \n",
    "    # Apply Haversine formula to calculate distance\n",
    "    print(\"üìè Calculating distances...\")\n",
    "    df_dist['Distance_KM'] = df_dist.apply(\n",
    "        lambda row: haversine(\n",
    "            row['Restaurant_latitude'],\n",
    "            row['Restaurant_longitude'],\n",
    "            row['Delivery_location_latitude'],\n",
    "            row['Delivery_location_longitude']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Also calculate Manhattan distance approximation (for urban areas)\n",
    "    # This can be useful for cities with grid-like road networks\n",
    "    df_dist['Lat_Diff_KM'] = abs(df_dist['Delivery_location_latitude'] - \n",
    "                                df_dist['Restaurant_latitude']) * 111.32\n",
    "    df_dist['Lon_Diff_KM'] = abs(df_dist['Delivery_location_longitude'] - \n",
    "                                df_dist['Restaurant_longitude']) * 111.32 * \\\n",
    "                                np.cos(np.radians(df_dist['Restaurant_latitude']))\n",
    "    \n",
    "    df_dist['Manhattan_Distance_KM'] = df_dist['Lat_Diff_KM'] + df_dist['Lon_Diff_KM']\n",
    "    \n",
    "    # Distance categories\n",
    "    df_dist['Distance_Category'] = pd.cut(\n",
    "        df_dist['Distance_KM'],\n",
    "        bins=[0, 2, 5, 10, 20, 100],\n",
    "        labels=['Very Close (<2km)', 'Close (2-5km)', 'Medium (5-10km)', \n",
    "                'Far (10-20km)', 'Very Far (>20km)']\n",
    "    )\n",
    "    \n",
    "    print(\" Distance calculation complete!\")\n",
    "    print(f\"\\n Distance Statistics (in KM):\")\n",
    "    print(f\"   Min Distance: {df_dist['Distance_KM'].min():.2f} km\")\n",
    "    print(f\"   Max Distance: {df_dist['Distance_KM'].max():.2f} km\")\n",
    "    print(f\"   Mean Distance: {df_dist['Distance_KM'].mean():.2f} km\")\n",
    "    print(f\"   Median Distance: {df_dist['Distance_KM'].median():.2f} km\")\n",
    "    \n",
    "    print(f\"\\n Distance Distribution:\")\n",
    "    print(df_dist['Distance_Category'].value_counts().sort_index())\n",
    "    \n",
    "    return df_dist\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: HANDLE MISSING VALUES\n",
    "# ============================================================================\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Identify and handle missing values in the dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 4: HANDLING MISSING VALUES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4.1: Check for missing values\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n Missing Values Before Cleaning:\")\n",
    "    missing_before = df_clean.isnull().sum()\n",
    "    missing_percent = (missing_before / len(df_clean)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_before,\n",
    "        'Missing_Percent': missing_percent\n",
    "    })\n",
    "    \n",
    "    # Only show columns with missing values\n",
    "    missing_df = missing_df[missing_df['Missing_Count'] > 0]\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df.sort_values('Missing_Percent', ascending=False))\n",
    "    else:\n",
    "        print(\" No missing values found!\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4.2: Handle specific columns with missing values\n",
    "    # ------------------------------------------------------------------------\n",
    "    if 'multiple_deliveries' in df_clean.columns:\n",
    "        print(f\"\\nüîß Handling 'multiple_deliveries' column...\")\n",
    "        # Fill NaN with 0 (assuming NaN means no multiple deliveries)\n",
    "        df_clean['multiple_deliveries'] = df_clean['multiple_deliveries'].fillna(0)\n",
    "        print(f\"   Filled {missing_before.get('multiple_deliveries', 0)} missing values with 0\")\n",
    "    \n",
    "    # Check for other potential missing values in key columns\n",
    "    key_columns = ['Delivery_person_Ratings', 'Weather_conditions', \n",
    "                   'Road_traffic_density', 'Type_of_order', 'Type_of_vehicle']\n",
    "    \n",
    "    for col in key_columns:\n",
    "        if col in df_clean.columns and df_clean[col].isnull().any():\n",
    "            if df_clean[col].dtype == 'object':\n",
    "                # For categorical, fill with mode\n",
    "                mode_val = df_clean[col].mode()[0]\n",
    "                df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "                print(f\"   Filled missing '{col}' with mode: {mode_val}\")\n",
    "            else:\n",
    "                # For numerical, fill with median\n",
    "                median_val = df_clean[col].median()\n",
    "                df_clean[col] = df_clean[col].fillna(median_val)\n",
    "                print(f\"   Filled missing '{col}' with median: {median_val:.2f}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4.3: Check for missing values after cleaning\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n Missing Values After Cleaning:\")\n",
    "    missing_after = df_clean.isnull().sum().sum()\n",
    "    \n",
    "    if missing_after == 0:\n",
    "        print(\"All missing values handled successfully!\")\n",
    "    else:\n",
    "        print(f\" Still have {missing_after} missing values\")\n",
    "        # Show remaining missing values\n",
    "        remaining_missing = df_clean.isnull().sum()\n",
    "        remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "        print(remaining_missing)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: ENCODE CATEGORICAL VARIABLES\n",
    "# ============================================================================\n",
    "\n",
    "def encode_categorical_variables(df):\n",
    "    \"\"\"\n",
    "    Encode categorical variables for machine learning\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" STEP 5: ENCODING CATEGORICAL VARIABLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5.1: Identify categorical columns\n",
    "    # ------------------------------------------------------------------------\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Remove ID columns and datetime columns that we've already processed\n",
    "    id_cols = ['ID', 'Delivery_person_ID']\n",
    "    date_cols = ['Order_Date', 'Time_Orderd', 'Time_Order_picked', \n",
    "                 'Order_DateTime', 'Pickup_DateTime', 'Order_DayOfWeek_Name']\n",
    "    \n",
    "    categorical_cols = [col for col in categorical_cols \n",
    "                       if col not in id_cols + date_cols]\n",
    "    \n",
    "    print(f\" Categorical columns to encode: {categorical_cols}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5.2: Create encoding dictionary for tracking\n",
    "    # ------------------------------------------------------------------------\n",
    "    encoding_info = {}\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5.3: Apply different encoding strategies\n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    # A) Ordinal Encoding for ordered categories\n",
    "    ordinal_mappings = {\n",
    "        'Road_traffic_density': {'Low': 0, 'Medium': 1, 'High': 2, 'Jam': 3},\n",
    "        'Vehicle_condition': {0: 0, 1: 1, 2: 2},  # Already numeric, but ensure it's treated as categorical\n",
    "        'multiple_deliveries': {0: 0, 1: 1, 2: 2, 3: 3}  # Ordered\n",
    "    }\n",
    "    \n",
    "    for col, mapping in ordinal_mappings.items():\n",
    "        if col in df_encoded.columns:\n",
    "            print(f\"\\nüî¢ Ordinal encoding '{col}': {mapping}\")\n",
    "            df_encoded[f'{col}_Encoded'] = df_encoded[col].map(mapping)\n",
    "            encoding_info[col] = {'type': 'ordinal', 'mapping': mapping}\n",
    "    \n",
    "    # B) One-Hot Encoding for nominal categories\n",
    "    nominal_cols = ['Weather_conditions', 'Type_of_order', \n",
    "                    'Type_of_vehicle', 'City', 'Festival', 'Time_Period']\n",
    "    \n",
    "    # Filter to only columns that exist in dataframe\n",
    "    nominal_cols = [col for col in nominal_cols if col in df_encoded.columns]\n",
    "    \n",
    "    print(f\"\\nüé≠ One-hot encoding for: {nominal_cols}\")\n",
    "    \n",
    "    for col in nominal_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            # Get dummies and prefix with column name\n",
    "            dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=True)\n",
    "            \n",
    "            # Add to dataframe\n",
    "            df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
    "            \n",
    "            # Store encoding info\n",
    "            categories = df_encoded[col].unique().tolist()\n",
    "            encoding_info[col] = {'type': 'one-hot', 'categories': categories}\n",
    "            \n",
    "            print(f\"   '{col}' ‚Üí {len(categories)} categories\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5.4: Create binary flags for important categories\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n Creating binary flags...\")\n",
    "    \n",
    "    # Festival binary flag\n",
    "    if 'Festival' in df_encoded.columns:\n",
    "        df_encoded['Is_Festival'] = (df_encoded['Festival'] == 'Yes').astype(int)\n",
    "    \n",
    "    # Bad weather flag\n",
    "    if 'Weather_conditions' in df_encoded.columns:\n",
    "        bad_weather = ['Fog', 'Stormy', 'Sandstorms', 'Windy']\n",
    "        df_encoded['Bad_Weather'] = df_encoded['Weather_conditions'].isin(bad_weather).astype(int)\n",
    "    \n",
    "    # Rush hour flag\n",
    "    df_encoded['Is_Rush_Hour'] = df_encoded['Order_Hour'].isin([8, 9, 12, 13, 17, 18, 19]).astype(int)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5.5: Display encoding summary\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n Encoding Summary:\")\n",
    "    for col, info in encoding_info.items():\n",
    "        if info['type'] == 'ordinal':\n",
    "            print(f\"   {col}: Ordinal ‚Üí {info['mapping']}\")\n",
    "        else:\n",
    "            print(f\"   {col}: One-Hot ‚Üí {len(info['categories'])} categories\")\n",
    "    \n",
    "    # Count new features created\n",
    "    original_cols = len(df.columns)\n",
    "    new_cols = len(df_encoded.columns)\n",
    "    print(f\"\\n Created {new_cols - original_cols} new encoded features\")\n",
    "    print(f\"   Total features: {new_cols}\")\n",
    "    \n",
    "    return df_encoded, encoding_info\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute all data preparation steps\n",
    "    \"\"\"\n",
    "    print(\" STARTING ZOMATO DELIVERY DATA PREPARATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 0. Load the data (replace with your actual file path)\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Assuming the data is saved as 'zomato_delivery_data.csv'\n",
    "    # If using the sample data, you might need to create this file first\n",
    "    \n",
    "    # For demonstration, let's create a sample dataframe from your sample rows\n",
    "    sample_data = {\n",
    "        'ID': ['0xcdcd', '0xd987', '0x2784', '0xc8b6', '0xdb64', '0x3af3', '0x3aab', '0x689b', '0x6f67', '0xc9cf', '0x36b8'],\n",
    "        'Delivery_person_ID': ['DEHRES17DEL01', 'KOCRES16DEL01', 'PUNERES13DEL03', 'LUDHRES15DEL02', 'KNPRES14DEL02', 'MUMRES15DEL03', 'MYSRES01DEL01', 'PUNERES20DEL01', 'HYDRES14DEL01', 'KOLRES15DEL03', 'PUNERES19DEL02'],\n",
    "        'Delivery_person_Age': [36, 21, 23, 34, 24, 29, 35, 33, 34, 21, 25],\n",
    "        'Delivery_person_Ratings': [4.2, 4.7, 4.7, 4.3, 4.7, 4.5, 4.0, 4.2, 4.9, 4.7, 4.1],\n",
    "        'Restaurant_latitude': [30.327968, 10.003064, 18.56245, 30.899584, 26.463504, 19.176269, 12.311072, 18.592718, 17.426228, 22.552672, 18.563934],\n",
    "        'Restaurant_longitude': [78.046106, 76.307589, 73.916619, 75.809346, 80.372929, 72.836721, 76.654878, 73.773572, 78.407495, 88.352885, 73.915367],\n",
    "        'Delivery_location_latitude': [30.397968, 10.043064, 18.65245, 30.919584, 26.593504, 19.266269, 12.351072, 18.702718, 17.496228, 22.582672, 18.643935],\n",
    "        'Delivery_location_longitude': [78.116106, 76.347589, 74.006619, 75.829346, 80.502929, 72.926721, 76.694878, 73.883572, 78.477495, 88.382885, 73.995367],\n",
    "        'Order_Date': ['12/2/2022', '13-02-2022', '4/3/2022', '13-02-2022', '14-02-2022', '2/4/2022', '1/3/2022', '16-03-2022', '20-03-2022', '15-02-2022', '16-03-2022'],\n",
    "        'Time_Orderd': ['21:55', '14:55', '17:30', '9:20', '19:50', '20:25', '14:55', '20:30', '20:40', '21:15', '20:20'],\n",
    "        'Time_Order_picked': ['22:10', '15:05', '17:40', '9:30', '20:05', '20:35', '15:10', '20:40', '20:50', '21:30', '20:25'],\n",
    "        'Weather_conditions': ['Fog', 'Stormy', 'Sandstorms', 'Sandstorms', 'Fog', 'Sandstorms', 'Windy', 'Sandstorms', 'Cloudy', 'Windy', 'Sandstorms'],\n",
    "        'Road_traffic_density': ['Jam', 'High', 'Medium', 'Low', 'Jam', 'Jam', 'High', 'Jam', 'Jam', 'Jam', 'Jam'],\n",
    "        'Vehicle_condition': [2, 1, 1, 0, 1, 2, 1, 2, 0, 0, 0],\n",
    "        'Type_of_order': ['Snack', 'Meal', 'Drinks', 'Buffet', 'Snack', 'Buffet', 'Meal', 'Snack', 'Snack', 'Meal', 'Snack'],\n",
    "        'Type_of_vehicle': ['motorcycle', 'motorcycle', 'scooter', 'motorcycle', 'scooter', 'electric_scooter', 'scooter', 'motorcycle', 'motorcycle', 'motorcycle', 'motorcycle'],\n",
    "        'multiple_deliveries': [3, 1, 1, 0, 1, 1, 1, 1, np.nan, 1, 2],\n",
    "        'Festival': ['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'],\n",
    "        'City': ['Metropolitian', 'Metropolitian', 'Metropolitian', 'Metropolitian', 'Metropolitian', 'Metropolitian', 'Metropolitian', 'Metropolitian', 'Metropolitian', 'Urban', 'Metropolitian'],\n",
    "        'Time_taken (min)': [46, 23, 21, 20, 41, 20, 33, 40, 41, 15, 36]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    print(\" Using sample data (11 rows from your example)\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 1. Clean date/time columns\n",
    "    # ------------------------------------------------------------------------\n",
    "    df = clean_datetime_columns(df)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 2. Calculate Haversine distance\n",
    "    # ------------------------------------------------------------------------\n",
    "    df = calculate_haversine_distance(df)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3. Handle missing values\n",
    "    # ------------------------------------------------------------------------\n",
    "    df = handle_missing_values(df)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 4. Encode categorical variables\n",
    "    # ------------------------------------------------------------------------\n",
    "    df, encoding_info = encode_categorical_variables(df)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 5. Display final prepared data\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA PREPARATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n Final Dataset Shape: {df.shape}\")\n",
    "    print(f\" Columns ({len(df.columns)} total):\")\n",
    "    \n",
    "    # Group columns by type\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\n    Numeric columns ({len(numeric_cols)}):\")\n",
    "    print(f\"      {numeric_cols[:10]}\")\n",
    "    if len(numeric_cols) > 10:\n",
    "        print(f\"      ... and {len(numeric_cols) - 10} more\")\n",
    "    \n",
    "    print(f\"\\n   Categorical columns ({len(categorical_cols)}):\")\n",
    "    print(f\"      {categorical_cols}\")\n",
    "    \n",
    "    print(f\"\\n   Datetime columns ({len(datetime_cols)}):\")\n",
    "    print(f\"      {datetime_cols}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 6. Save the prepared data\n",
    "    # ------------------------------------------------------------------------\n",
    "    output_file = 'zomato_delivery_prepared.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n Prepared data saved to: {output_file}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # 7. Show key features for modeling\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY FEATURES READY FOR MODELING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show correlation with target variable\n",
    "    if 'Time_taken (min)' in df.columns:\n",
    "        print(\"\\n Correlation with Target (Time_taken):\")\n",
    "        numeric_features = [col for col in numeric_cols \n",
    "                           if col != 'Time_taken (min)' and \n",
    "                           not any(x in col for x in ['latitude', 'longitude', 'Diff'])]\n",
    "        \n",
    "        correlations = {}\n",
    "        for col in numeric_features:\n",
    "            if col in df.columns:\n",
    "                corr = df[col].corr(df['Time_taken (min)'])\n",
    "                correlations[col] = corr\n",
    "        \n",
    "        # Sort by absolute correlation\n",
    "        sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        print(\"\\nTop correlated features:\")\n",
    "        for feature, corr in sorted_corr[:10]:\n",
    "            print(f\"   {feature:30} : {corr:+.3f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# RUN THE SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the data preparation pipeline\n",
    "    prepared_df = main()\n",
    "    \n",
    "    # Display the first few rows of prepared data\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" SAMPLE OF PREPARED DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Select key columns to display\n",
    "    key_columns = [\n",
    "        'Distance_KM', 'Preparation_Time_Min', 'Order_Hour', \n",
    "        'Is_Weekend', 'Road_traffic_density_Encoded',\n",
    "        'Bad_Weather', 'Is_Rush_Hour', 'Time_taken (min)'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only columns that exist\n",
    "    display_cols = [col for col in key_columns if col in prepared_df.columns]\n",
    "    \n",
    "    if display_cols:\n",
    "        print(prepared_df[display_cols].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
