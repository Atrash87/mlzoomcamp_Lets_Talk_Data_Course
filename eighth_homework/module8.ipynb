{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b660723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "from torchsummary import summary\n",
    "print(\"PyTorch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa3a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb6cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "import numpy as np, random, torch\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903036d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classes: ['Straight', 'Wavy', 'curly', 'dreadlocks', 'kinky']\n",
      "Using classes for binary classification: ['Straight', 'curly']\n"
     ]
    }
   ],
   "source": [
    "# Set path to your data folder (relative to notebook). Adjust if needed.\n",
    "DATA_DIR = \"data\"  # change only if your folder is elsewhere\n",
    "\n",
    "# Find class folders\n",
    "classes = sorted([d.name for d in Path(DATA_DIR).iterdir() if d.is_dir()])\n",
    "print(\"Found classes:\", classes)\n",
    "\n",
    "# Prefer these two if available (case-sensitive)\n",
    "preferred = []\n",
    "if \"Straight\" in classes: preferred.append(\"Straight\")\n",
    "if \"curly\" in classes: preferred.append(\"curly\")\n",
    "\n",
    "# Fallback: pick first two\n",
    "if len(preferred) < 2:\n",
    "    if len(classes) >= 2:\n",
    "        chosen = classes[:2]\n",
    "    else:\n",
    "        raise RuntimeError(\"Not enough class folders found in data/\")\n",
    "else:\n",
    "    chosen = preferred[:2]\n",
    "\n",
    "print(\"Using classes for binary classification:\", chosen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9183ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageFolder classes: ['Straight', 'Wavy', 'curly', 'dreadlocks', 'kinky']\n",
      "Train size: 800 Validation size: 201\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200,200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((200,200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Use ImageFolder but restrict to the two chosen classes\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transforms)\n",
    "# Map class index -> class name\n",
    "all_classes = full_dataset.classes\n",
    "print(\"ImageFolder classes:\", all_classes)\n",
    "\n",
    "# Get indices of samples belonging to chosen classes\n",
    "chosen_indices = [i for i,(path,label) in enumerate(full_dataset.samples) if all_classes[label] in chosen]\n",
    "if len(chosen_indices) == 0:\n",
    "    raise RuntimeError(\"No images found for chosen classes. Check DATA_DIR and class names.\")\n",
    "\n",
    "# Build a Subset dataset and remap labels to 0/1\n",
    "subset = Subset(full_dataset, chosen_indices)\n",
    "\n",
    "# To remap labels to 0/1 we create a small wrapper dataset\n",
    "from torch.utils.data import Dataset\n",
    "class BinarySubset(Dataset):\n",
    "    def __init__(self, subset, chosen_names, transform=None):\n",
    "        self.subset = subset\n",
    "        self.chosen = chosen_names\n",
    "        self.transform = transform\n",
    "        # capture mapping from original class idx to class name\n",
    "        self.orig_classes = full_dataset.classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, orig_label = self.subset[idx]  # orig_label is original class index\n",
    "        orig_name = self.orig_classes[orig_label]\n",
    "        new_label = float(self.chosen.index(orig_name))  # 0.0 or 1.0\n",
    "        return img, torch.tensor(new_label, dtype=torch.float32)\n",
    "\n",
    "# Create train/validation split (80/20)\n",
    "n = len(subset)\n",
    "indices = list(range(n))\n",
    "random.shuffle(indices)\n",
    "split = int(0.8 * n)\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_dataset = Subset(subset, train_idx)\n",
    "val_dataset = Subset(subset, val_idx)\n",
    "\n",
    "train_dataset = BinarySubset(train_dataset, chosen, transform=None)\n",
    "validation_dataset = BinarySubset(val_dataset, chosen, transform=None)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset), \"Validation size:\", len(validation_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a116b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8449bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HairNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (fc1): Linear(in_features=313632, out_features=64, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 198, 198]             896\n",
      "         MaxPool2d-2           [-1, 32, 99, 99]               0\n",
      "            Linear-3                   [-1, 64]      20,072,512\n",
      "            Linear-4                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 20,073,473\n",
      "Trainable params: 20,073,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.46\n",
      "Forward/backward pass size (MB): 11.96\n",
      "Params size (MB): 76.57\n",
      "Estimated Total Size (MB): 89.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class HairNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)  # no padding, stride=1 default\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        # We'll create fc1 dynamically after knowing conv output size\n",
    "        self.fc1 = None\n",
    "        self.fc2 = nn.Linear(64, 1)  # placeholder; we'll replace if needed\n",
    "\n",
    "    def _get_conv_output(self, shape=(3,200,200)):\n",
    "        bs = 1\n",
    "        input = torch.zeros(bs, *shape)\n",
    "        out = F.relu(self.conv1(input))\n",
    "        out = self.pool(out)\n",
    "        return int(np.prod(out.size()[1:]))\n",
    "\n",
    "    def build_fc(self):\n",
    "        conv_out_size = self._get_conv_output()\n",
    "        self.fc1 = nn.Linear(conv_out_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # NOTE: we'll use BCEWithLogitsLoss -> no sigmoid here\n",
    "        return x\n",
    "\n",
    "model = HairNet().to(device)\n",
    "model.build_fc()\n",
    "print(model)\n",
    "# optional: show summary\n",
    "try:\n",
    "    summary(model, input_size=(3,200,200))\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707761b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40226103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Loss: 0.6391  Acc: 0.6300  ValLoss: 0.6074  ValAcc: 0.6368\n",
      "Epoch 2/10  Loss: 0.5814  Acc: 0.6737  ValLoss: 0.5247  ValAcc: 0.7313\n",
      "Epoch 3/10  Loss: 0.5571  Acc: 0.7025  ValLoss: 0.4970  ValAcc: 0.7811\n",
      "Epoch 4/10  Loss: 0.4971  Acc: 0.7350  ValLoss: 0.4925  ValAcc: 0.7612\n",
      "Epoch 5/10  Loss: 0.4478  Acc: 0.7675  ValLoss: 0.5519  ValAcc: 0.7015\n",
      "Epoch 6/10  Loss: 0.3686  Acc: 0.8200  ValLoss: 0.5430  ValAcc: 0.7114\n",
      "Epoch 7/10  Loss: 0.3621  Acc: 0.8300  ValLoss: 0.5529  ValAcc: 0.6866\n",
      "Epoch 8/10  Loss: 0.3653  Acc: 0.8512  ValLoss: 0.5998  ValAcc: 0.7313\n",
      "Epoch 9/10  Loss: 0.2529  Acc: 0.9137  ValLoss: 0.4848  ValAcc: 0.8060\n",
      "Epoch 10/10  Loss: 0.2273  Acc: 0.9125  ValLoss: 0.5660  ValAcc: 0.7761\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device).unsqueeze(1)  # shape (B,1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f}  ValLoss: {val_epoch_loss:.4f}  ValAcc: {val_epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66232b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracies (per epoch): [0.63    0.67375 0.7025  0.735   0.7675  0.82    0.83    0.85125 0.91375\n",
      " 0.9125 ]\n",
      "Training losses (per epoch): [0.63905114 0.58138034 0.55706192 0.49714154 0.44783388 0.36857385\n",
      " 0.36207833 0.36533775 0.25288861 0.22730693]\n",
      "Q3 - median training acc: 0.7937\n",
      "Q4 - std training loss: 0.1314\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_accs = np.array(history['acc'])\n",
    "train_losses = np.array(history['loss'])\n",
    "\n",
    "q3_median_acc = float(np.median(train_accs))\n",
    "q4_std_loss = float(np.std(train_losses, ddof=0))  # population std to match typical choices\n",
    "\n",
    "print(\"Training accuracies (per epoch):\", train_accs)\n",
    "print(\"Training losses (per epoch):\", train_losses)\n",
    "print(f\"Q3 - median training acc: {q3_median_acc:.4f}\")\n",
    "print(f\"Q4 - std training loss: {q4_std_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f71e3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug Epoch 1/10  Loss: 0.6779  Acc: 0.6150  ValLoss: 0.5727  ValAcc: 0.6219\n",
      "Aug Epoch 2/10  Loss: 0.5935  Acc: 0.6863  ValLoss: 0.5442  ValAcc: 0.6766\n",
      "Aug Epoch 3/10  Loss: 0.5652  Acc: 0.7100  ValLoss: 0.5566  ValAcc: 0.6915\n",
      "Aug Epoch 4/10  Loss: 0.5491  Acc: 0.7063  ValLoss: 0.5325  ValAcc: 0.7015\n",
      "Aug Epoch 5/10  Loss: 0.5478  Acc: 0.7063  ValLoss: 0.5617  ValAcc: 0.7065\n",
      "Aug Epoch 6/10  Loss: 0.5342  Acc: 0.7400  ValLoss: 0.5260  ValAcc: 0.7264\n",
      "Aug Epoch 7/10  Loss: 0.5310  Acc: 0.7150  ValLoss: 0.4763  ValAcc: 0.7413\n",
      "Aug Epoch 8/10  Loss: 0.5115  Acc: 0.7412  ValLoss: 0.5146  ValAcc: 0.7313\n",
      "Aug Epoch 9/10  Loss: 0.4994  Acc: 0.7400  ValLoss: 0.4702  ValAcc: 0.7711\n",
      "Aug Epoch 10/10  Loss: 0.5035  Acc: 0.7412  ValLoss: 0.4951  ValAcc: 0.7612\n"
     ]
    }
   ],
   "source": [
    "aug_train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(50),\n",
    "    transforms.RandomResizedCrop(200, scale=(0.9,1.0), ratio=(0.9,1.1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((200,200)),  # ensure final size (some transforms may change it)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Recreate full_dataset but with augmentation for training\n",
    "full_dataset_aug = datasets.ImageFolder(DATA_DIR, transform=aug_train_transforms)\n",
    "subset_aug = Subset(full_dataset_aug, chosen_indices)  # same indices selection as before\n",
    "\n",
    "# re-split: we must use the same train/val split indices used earlier\n",
    "train_subset_aug = Subset(subset_aug, train_idx)\n",
    "val_subset_aug = Subset(subset_aug, val_idx)\n",
    "\n",
    "train_dataset_aug = BinarySubset(train_subset_aug, chosen)\n",
    "validation_dataset_aug = BinarySubset(val_subset_aug, chosen)\n",
    "\n",
    "train_loader_aug = DataLoader(train_dataset_aug, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader_aug = DataLoader(validation_dataset_aug, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Continue training for 10 more epochs\n",
    "num_epochs_aug = 10\n",
    "aug_history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs_aug):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader_aug:\n",
    "        images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset_aug)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    aug_history['loss'].append(epoch_loss)\n",
    "    aug_history['acc'].append(epoch_acc)\n",
    "\n",
    "    # validation (test) on the same validation dataset (with transform matching test_transforms)\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader_aug:\n",
    "            images, labels = images.to(device), labels.to(device).unsqueeze(1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(validation_dataset_aug)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    aug_history['val_loss'].append(val_epoch_loss)\n",
    "    aug_history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Aug Epoch {epoch+1}/{num_epochs_aug}  Loss: {epoch_loss:.4f}  Acc: {epoch_acc:.4f}  ValLoss: {val_epoch_loss:.4f}  ValAcc: {val_epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c40c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation-phase validation losses (10 epochs): [0.57266151 0.54416775 0.55656575 0.53245739 0.56171739 0.52598743\n",
      " 0.47634739 0.51464022 0.47017972 0.49509052]\n",
      "Augmentation-phase validation accs (10 epochs): [0.62189055 0.67661692 0.69154229 0.70149254 0.70646766 0.72636816\n",
      " 0.74129353 0.73134328 0.77114428 0.76119403]\n",
      "Q5 - mean test loss (10 aug epochs): 0.5250\n",
      "Q6 - avg test acc (epochs 6-10): 0.7463\n"
     ]
    }
   ],
   "source": [
    "aug_val_losses = np.array(aug_history['val_loss'])\n",
    "aug_val_accs = np.array(aug_history['val_acc'])\n",
    "\n",
    "q5_mean_test_loss = float(np.mean(aug_val_losses))  # mean over all 10 augmentation epochs\n",
    "# average of test accuracy for last 5 epochs (6..10) -> indices 5:10\n",
    "q6_avg_last5_test_acc = float(np.mean(aug_val_accs[5:10]))\n",
    "\n",
    "print(\"Augmentation-phase validation losses (10 epochs):\", aug_val_losses)\n",
    "print(\"Augmentation-phase validation accs (10 epochs):\", aug_val_accs)\n",
    "print(f\"Q5 - mean test loss (10 aug epochs): {q5_mean_test_loss:.4f}\")\n",
    "print(f\"Q6 - avg test acc (epochs 6-10): {q6_avg_last5_test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
